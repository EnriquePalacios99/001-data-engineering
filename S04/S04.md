# MLOps Course - Python Essentials for MLOps

En este apartado 


## Onle Analytical Processing - OLAP 

Ejemplo: 

Startup - Financiera 

Bases de datos ¿Quée pasa si usamos este motor? ¿Qué problema encontrariamos?

- Al realizar consultas se caería 

**Recomendación:**

- Separar el motor OLTP
- Otro motor para realizar consultas 


OLTP y OLAP - Características 

| Característica      | OLTP (Online Transaction Processing) | OLAP (Online Analytical Processing) |
|----------------------|-------------------------------------|------------------------------------|
| **Objetivo** | Operaciones del día a día          | Análisis e informes               |
| **Tipos de datos** | Transaccionales                   | Agregados e históricos             |
| **Frecuencia de acceso** | Alta                                | Baja (pero consultas complejas)    |
| **Diseño** | Normalizado                         | Desnormalizado (modelos star/snowflake) |



Normalizado: data normalizada detallado para garantizar que las transacciones sena rápidas 
Desnormalizado: ventas del día cuanto se ha vendido 


## Datawerehouse 

¿Qué es un Datawerehouse?

""Cita""  Bill Innom 


"Cita"  Ralph Kimball 


## Caractesisticas 

Datos estructurados y de calidad
* Optimizado para consultas analíticas (OLAP)
* Modelado dimensional (star/snowflake
schema)
* Batch ingestion (datos periódicos) -> cuando llega información constante (inmediata - OLTP)
• Centralizado y gobernado: tenemos la información importante de la empresa y existe un gobierno establecido 

# Efoques de Modelos de Datos: 

| Criterio        | Inmon (Top-down)           | Kimball (Bottom-up)            |
|-----------------|----------------------------|--------------------------------|
| **Filosofía** | Top-down (empresa): Es a partir del conocimiento de toda la empresa desde lo más alto hasta los detalles         | Bottom-up (necesidad): ahora el equipo tiene esta información ahora llega una nueva y cómo lo mejoro        |
| **Modelo** | 3NF corporativa: información independiente            | Dimensional (star/snowflake)   |
| **Implementación** | Lenta pero robusta        | Rápida y enfocada              |
| **Ideal para** | Gobierno y consistencia    | Agilidad y usuarios finales    |


##  Dimensional Data Models 

Tabla se Hechos -> Más dimensiones 


Sales (Tabla de Hechos - Fact Table)
Products(Dimensión)
Departamentos()


* **Modelos Snowflake**

Productos: Desdobla en más (Product segments)


**Data Werehouse Pros y Contras**

| Ventajas                                                   | Desventajas                                                      |
|------------------------------------------------------------|-------------------------------------------------------------------|
| Centralización de la información: integra datos de múltiples fuentes. | Costo inicial elevado: implementación y mantenimiento pueden ser costosos. |
| Mejora la toma de decisiones: facilita el análisis histórico y tendencias. | Complejidad técnica: requiere diseño, modelado y conocimientos especializados. |
| Alta velocidad en consultas analíticas: optimizado para OLAP. | No apto para transacciones operativas: no reemplaza un sistema OLTP. |
| Calidad y consistencia de datos: validación, limpieza y reglas unificadas. | Tiempo de carga: ETL puede ser lento si no está bien optimizado.         |
| Soporte para BI y reportes: fácil integración con Power BI, Tableau, etc. | Rigidez ante cambios: agregar nuevas fuentes o métricas requiere rediseño. |
| Seguridad y control de acceso: políticas claras sobre quién accede a qué. | Latencia: los datos no siempre están en tiempo real.                |





**¿Qué es un Datalake?**

Adquirir infomación de diferentes fuentes. Se puede guardar imagenes. Datos no estructurados y estructurados

*Caracteristicas*

* Flexible: acepta datos estructurados, semi y no
estructurados.
• Ideal para ciencia de datos y machine learning.
• Basado en almacenamiento distribuido
• Diseñado para grandes volúmenes de datos.
• Bajo costo de almacenamiento. (Barato)
• Alta escalabilidad


**¿Cómo se organizan los datos en un Datalake?**

| Característica             | Raw (Landing Zone / Bronze Layer)                 | Cleansed (Staging Zone / Silver Layer)        | Curated (Business Zone / Gold Layer)           |
|----------------------------|---------------------------------------------------|---------------------------------------------|----------------------------------------------|
| **Ingesta de datos** | En su forma original (sin procesar)              | Datos validados, transformados y estandarizados | Datos listos para análisis y modelado        |
| **Estado de los datos** | Sin validar, sin transformar, sin estandarizar   | Validados, transformados y estandarizados     | Modelados y con valor de negocio             |
| **Procesamiento principal**| Ninguno (ingesta directa)                       | Aplicación de reglas de calidad y transformaciones | Modelado para facilitar el consumo y análisis |
| **Propósito principal** | Almacenamiento inicial de los datos fuente      | Preparación de los datos para el análisis       | Soporte directo para informes, dashboards y análisis |


**¿Qué pasa si todo se almacena sin gobernanza?**


## **Ventajas y desventajas**


| Ventajas                                                                 | Desventajas                                                                    |
|--------------------------------------------------------------------------|---------------------------------------------------------------------------------|
| Almacena cualquier tipo de dato (estructurado, semi, no estructurado)     | Puede volverse un data swamp si no se gestiona bien                              |
| Escalabilidad prácticamente ilimitada (especialmente en la nube)         | Requiere buen gobierno de datos para mantener calidad y trazabilidad             |
| Bajo costo por almacenamiento (comparado con un DWH tradicional)        | La consulta de datos puede ser más lenta y menos optimizada                      |
| Flexibilidad para aplicar machine learning y análisis avanzado           | No tiene modelo de datos predefinido (más complejo para usuarios no técnicos)   |
| Ideal para ingesta masiva y en tiempo real de múltiples fuentes          | Se necesita infraestructura técnica para transformar y preparar los datos        |
| Compatible con múltiples herramientas de análisis y visualización        | Puede haber redundancia si no se controla bien la duplicación de datos          |


**Qué es un lakehouse**

- Elimina la duplicación de datos entre sistemas (Lakes y Warehouses)
- Reduce costos y complejidad de procesos de carga tradicionales
- Disminuye la latencia entre la captura de datos y el análisis
- Permite procesar datos estructurados, semi-estructurados y no estructurados en
una misma arquitectura
- Mejora la gobernanza de datos mediante catálogos y control de versiones
- Facilita el trabajo de equipos multidisciplinarios (data engineers, analysts,
científicos de datos) sobre una fuente común
- Escala horizontalmente sin necesidad de rediseñar la arquitectura

**Caracteristicas**

Almacenamiento en formatos abiertos
(ej. Parquet, Avro, JSON, csv).
• Transacciones ACID
(Atomicidad, Consistencia, Aislamiento y Durabilidad)
• Gestión de metadatos
• Soporte para machine learning y BI
• Separación de almacenamiento y cómputo 
• Escalabilidad y costo eficiente


**Capacidades Técnicas Clave del Lakehouse**

Transacciones ACID: Garantiza consistencia y control de concurrencia
• Time Travel: Permite acceder a versiones históricas de los datos
• Catálogo de Metadatos: Gestión centralizada para múltiples motores
• Separación de cómputo y almacenamiento: Escalabilidad independiente
• Soporte nativo para ML: Entrenamiento sin mover ni duplicar datos
Capacidades Técnicas Clave del Lakehouse
33
Ejemplo: Databricks + Delta Lake


**Formatos comunes en un Lakehouse**

| Tipo de Formato | Ejemplo      | Características Clave                               | Casos de Uso Típicos                      |
|-----------------|--------------|-----------------------------------------------------|-------------------------------------------|
| Columnar        | Parquet, Delta Lake | - Alta compresión<br>- Lectura selectiva de columnas<br>- Ideal para grandes volúmenes de datos | Consultas analíticas, dashboards, machine learning |
| Por filas        | Avro         | - Escritura rápida y secuencial<br>- Buen para procesamiento de eventos<br>- Autodescriptivo | Streaming, ingestión de logs, sistemas de mensajería | 


**Diseño de componente dimensional para un DWH
**


**Objetivo:**
• Mejorar la capacidad de análisis de ventas a nivel regional, de producto y de
cliente, a fin de identificar patrones de consumo, oportunidades de crecimiento y
optimizar decisiones comerciales en mercados internacionales.


Diseño de componente dimensional para un DWH
40
Justificación:
Se requiere una estructura analítica que permita responder rápidamente preguntas
como:
• ¿Qué productos se venden más en cada región?
• ¿Qué clientes generan mayores ingresos?
• ¿Cómo varían las ventas día a día?


**Diseño de componente dimensional para un DWH
**

**Solución propuesta:**
• Construcción de un componente dimensional como parte del Data Warehouse
empresarial, permitiendo análisis flexibles mediante un esquema en estrella (Star
Schema).


-------
**Libros recomendados**